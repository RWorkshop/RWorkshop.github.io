\documentclass[caret-main.tex]{subfiles}
\begin{document}
%- http://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/#Random_Forests_an_Ensemble_Method
%\section{Random Forests}
%
%The random forest (Breiman, 2001) is an ensemble approach that can also be thought of as a form of nearest neighbor predictor.
%
%Ensembles are a divide-and-conquer approach used to improve performance. The main principle behind ensemble methods is that a group of “weak learners” can come together to form a “strong learner”. The figure below (taken from here) provides an example. Each classifier, individually, is a “weak learner,” while all the classifiers taken together are a “strong learner”.
%
%The data to be modeled are the blue circles. We assume that they represent some underlying function plus noise. Each individual learner is shown as a gray curve. Each gray curve (a weak learner) is a fair approximation to the underlying data. The red curve (the ensemble “strong learner”) can be seen to be a much better approximation to the underlying data.
%%--------------------------------------------------------------------------------------%
%\newpage
%
%
%Random forests are an ensemble learning method for classification (and regression) that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees.
%
% The algorithm for inducing a random forest was developed by Leo Breiman[1] and Adele Cutler,[2] and "Random Forests" is their trademark. The term came from random decision forests that was first proposed by Tin Kam Ho of Bell Labs in 1995. 
% 
% The method combines Breiman's "bagging" idea and the random selection of features, introduced independently by Ho[3][4] and Amit and Geman[5] in order to construct a collection of decision trees with controlled variance.

\section{RandomForest with R}
\begin{framed}
\begin{verbatim}
library(randomForest)
 
# download Titanic Survivors data
data <- read.table("http://math.ucdenver.edu/RTutorial/titanic.txt", h=T, sep="\t")
# make survived into a yes/no
data$Survived <- as.factor(ifelse(data$Survived==1, "yes", "no"))                 
 
# split into a training and test set
idx <- runif(nrow(data)) <= .75
data.train <- data[idx,]
data.test <- data[-idx,]
\end{verbatim}
\end{framed} 
Train a random forest
\begin{framed}
\begin{verbatim} 

rf <- randomForest(Survived ~ PClass + Age + Sex, 
             data=data.train, importance=TRUE, na.action=na.omit)
\end{verbatim}
\end{framed} 
How important is each variable in the model?
\begin{framed}
\begin{verbatim}
imp <- importance(rf)
o <- order(imp[,3], decreasing=T)
imp[o,]
#             no      yes MeanDecreaseAccuracy MeanDecreaseGini
#Sex    51.49855 53.30255             55.13458         63.46861
#PClass 25.48715 24.12522             28.43298         22.31789
#Age    20.08571 14.07954             24.64607         19.57423
\end{verbatim}
\end{framed} 
Display the confusion matrix
\begin{framed}
\begin{verbatim} 
# confusion matrix [[True Neg, False Pos], [False Neg, True Pos]]
table(data.test$Survived, predict(rf, data.test),
  dnn=list("actual", "predicted"))
#      predicted
#actual  no yes
#   no  427  16
#   yes 117 195
\end{verbatim}
\end{framed}
\newpage
%I'm running a random forest model using R's caret package, and running varImp on the returned object gives me the averaged variable importance across the number of bootstrap iterations. However, I would rather assess variable importance for each iteration. Is this possible using the caret package?
%
%Reproducible example:
\begin{verbatim}
library(caret)
mod <- train(Species ~ ., data = iris,
         method = "cforest",
         controls = cforest_unbiased(ntree = 10))
varImp(mod)
\end{verbatim}
returns:
\begin{verbatim}
cforest variable importance
Overall
Petal.Width  100.0000
Petal.Length  86.6279
Sepal.Length   0.5814
Sepal.Width    0.0000 
\end{verbatim}
%what I'm interested in is rather a list of length=number of bootstrap resamples with variable importance for each iteration. This might be possible using some combination of returnResamp = "all" and a custom summaryFunction but I'm not wise enough to know.


\end{document}
