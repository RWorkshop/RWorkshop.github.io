\documentclass[caret-main.tex]{subfiles}
\begin{document}
\section{Resampling Methods}
The resampling methods used by \textbf{caret} are:
\begin{itemize}
\item bootstrapping, 
\item k-fold crossvalidation,
\item leave-one-out cross-validation,
\item  leave-group-out cross-validation (i.e., repeated
splits without replacement).
\end{itemize}

%http://www.obgyn.cam.ac.uk/cam-only/statsbook/stcart.html
\subsection{Avoiding Over-Fitting}

A major issue that arises when applying regression or classification trees to "real" data with much random error noise concerns the decision when to stop splitting. For example, if you had a data set with 10 cases, and performed 9 splits (determined 9 if-then conditions), you could perfectly predict every single case. In general, if you only split a sufficient number of times, eventually you will be able to "predict" ("reproduce" would be the more appropriate term here) your original data (from which you determined the splits). Of course, it is far from clear whether such complex results (with many splits) will replicate in a sample of new observations; most likely they will not.

This general issue is also discussed in the literature on tree classification and regression methods, as well as neural networks, under the topic of \textbf{overlearning} or \textbf{overfitting}.
 If not stopped, the tree algorithm will ultimately "extract" all information from the data, including information that is not and cannot be predicted in the population with the current set of predictors, i.e., random or noise variation. 

The general approach to addressing this issue is first to stop generating new split nodes when subsequent splits only result in very little overall improvement of the prediction. For example, if you can predict 90\% of all cases correctly from 10 splits, and 90.1\% of all cases from 11 splits, then it obviously makes little sense to add that 11th split to the tree. There are many such criteria for automatically stopping the splitting (tree-building) process.

\subsubsection{Pruning}
Once the tree building algorithm has stopped, it is always useful to further evaluate the quality of the prediction of the current tree in samples of observations that did not participate in the original computations. These methods are used to "prune back" the tree, i.e., to eventually (and ideally) select a simpler tree than the one obtained when the tree building algorithm stopped, but one that is equally as accurate for predicting or classifying "new" observations.

\subsubsection{Crossvalidation}
One approach is to apply the tree computed from one set of observations (learning sample) to another completely independent set of observations (testing sample). If most or all of the splits determined by the analysis of the learning sample are essentially based on "random noise," then the prediction for the testing sample will be very poor. Hence one can infer that the selected tree is not very good (useful), and not of the "right size."

\subsubsection{V-fold crossvalidation} 
Continuing further along this line of reasoning (described in the context of crossvalidation above), why not repeat the analysis many times over with different randomly drawn samples from the data, for every tree size starting at the root of the tree, and applying it to the prediction of observations from randomly selected testing samples. Then use (interpret, or accept as your final result) the tree that shows the best average accuracy for cross-validated predicted classifications or predicted values. 

In most cases, this tree will not be the one with the most terminal nodes, i.e., the most complex tree. This method for pruning a tree, and for selecting a smaller tree from a sequence of trees, can be very powerful, and is particularly useful for smaller data sets. It is an essential step for generating useful (for prediction) tree models, and because it can be computationally difficult to do, this method is often not found in tree classification or regression software.
\newpage
\section{Cross Validation}
Bias Variance Trade-off \textit{http://scott.fortmann-roe.com/docs/BiasVariance.html}
\begin{itemize}
\item In a prediction problem, a model is usually given a dataset of known data 
on which training is run (\textit{training dataset}), and a dataset of unknown data (or \textit{first seen data/ testing dataset}) against which testing the model is performed.
\item Cross-validation is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. 
\item The goal of cross validation is to define a dataset to "test" the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting, give an insight on how the model will generalize to an independent data set (i.e., an unknown dataset, for instance from a real problem), etc.
\item Cross-validation is important in guarding against testing hypotheses suggested by the data (called "Type III errors"), especially where further samples 
are hazardous, costly or impossible to collect 
\end{itemize}
\newpage
\subsubsection{K-fold cross validation}
\begin{itemize}
\item In k-fold cross-validation, the original data set is randomly partitioned into $k$ equal size subsamples. 
\item Of the $k$ subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k - 1 subsamples are used as training data. 
\item The cross-validation process is then repeated k times (the folds), with each of the $k$ subsamples used exactly once as the validation data. \item The $k$ results from the folds can then be averaged (or otherwise combined) to produce a single estimation.
\item The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once. 
\end{itemize}
\subsubsection{Choosing K - Bias and Variance}
In general, when using k-fold cross validation, it seems to be the case that:
\begin{itemize}
\item A larger k will produce an estimate with smaller bias but potentially higher variance (on top of being computationally expensive)
\item A smaller k will lead to a smaller variance but may lead to a a biased estimate.
\end{itemize}

\subsubsection{Leave-One-Out Cross-Validation}
\begin{itemize}
\item As the name suggests, leave-one-out cross-validation (LOOCV) involves using a single observation from the original sample as the validation data, and the remaining observations as the training data. 
\item This is repeated such that each observation in the sample is used once as the validation data. 
\item This is the same as a K-fold cross-validation with K being equal to the number of observations in the original sampling, i.e. \textbf{K=n}.
\end{itemize}

\newpage


\end{document}
