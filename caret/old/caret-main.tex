\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage{subfiles}

\usepackage[utf8]{inputenc}
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} 
\usepackage{framed}
\usepackage{graphicx} 
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} 
\usepackage{subfig} 
%-----------------------------------------------------%
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape}
%--------------------------------------------------------------------------------------%
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%--------------------------------------------------------------------------------------%

\title{Building Predictive Models with the \textit{caret} package}

\author{Dublin \texttt{R}}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle
\tableofcontents
%-----------------------------------------------------------------------------------------------%
\newpage

\subfile{caret-intro}
%------------------------------------------------------------------------------------------------%
\section{Classification and Regression Trees}
C\&RT, a recursive partitioning method, builds classification and regression trees for predicting continuous dependent variables (regression) and categorical predictor variables (classification). The classic C\&RT algorithm was popularized by Breiman et al. \textit{(Breiman, Friedman, Olshen, \& Stone, 1984; see also Ripley, 1996).}

\subsection{Advantages of C\&RT Methods}

% https://www.statsoft.com/Textbook/Classification-and-Regression-Trees

As mentioned earlier, there are a large number of methods that an analyst can choose from when analyzing classification or regression problems. Tree classification techniques, when they "work" and produce accurate predictions or predicted classifications based on few logical if-then conditions, have a number of advantages over many of those alternative techniques.

Simplicity of results. In most cases, the interpretation of results summarized in a tree is very simple. This simplicity is useful not only for purposes of rapid classification of new observations (it is much easier to evaluate just one or two logical conditions, than to compute classification scores for each possible group, or predicted values, based on all predictors and using possibly some complex nonlinear model equations), but can also often yield a much simpler "model" for explaining why observations are classified or predicted in a particular manner (e.g., when analyzing business problems, it is much easier to present a few simple if-then statements to management, than some elaborate equations).

Tree methods are nonparametric and nonlinear. The final results of using tree methods for classification or regression can be summarized in a series of (usually few) logical if-then conditions (tree nodes). Therefore, there is no implicit assumption that the underlying relationships between the predictor variables and the dependent variable are linear, follow some specific non-linear link function], or that they are even monotonic in nature. 

For example, some continuous outcome variable of interest could be positively related to a variable Income if the income is less than some certain amount, but negatively related if it is more than that amount (i.e., the tree could reveal multiple splits based on the same variable Income, revealing such a non-monotonic relationship between the variables). 

Thus, tree methods are particularly well suited for data mining tasks, where there is often little a priori knowledge nor any coherent set of theories or predictions regarding which variables are related and how. In those types of data analyses, tree methods can often reveal simple relationships between just a few variables that could have easily gone unnoticed using other analytic techniques.

%-------------------------------------------------------------------------------------------%
\newpage

\subsection{Workflow}
Modeling in R generally follows the same workflow:

\begin{enumerate} 
\item Create the model using the basic function:
\begin{verbatim}
fit <- knn(trainingData, outcome, k = 5)
\end{verbatim}
\item Assess the properties of the model using \texttt{print}, \texttt{plot}, \texttt{summary} or
other methods
\item  Predict outcomes for samples using the \texttt{predict} method:
\begin{verbatim}
predict(fit, newSamples).
\end{verbatim}
The model can be used for prediction without changing the original model
object.
\end{enumerate}
%-------------------------------------------------------------------------------------------%


% \subfile{caret-classtrees}
\subfile{caret-regresstrees}
\subfile{caret-confusionmatrix}
%-------------------------------------------------------------------------------------------%


\subsection{Assessing The Performance of a Predictive Model}


\textbf{\textit{caret}} has several functions that can be used to describe the performance of classification models.


 For binary classifiation models, the functions \texttt{sensitivity}, \texttt{specificity},
\texttt{posPredValue} and \texttt{negPredValue} can be used to characterize performance.

By default, the first level of the outcome factor is used to define the ``positive"
result, although this can be changed.




\newpage
\subsection{Appraising Regression Models}
\begin{itemize}
\item For regression models, performance is calculated using the \textit{root
mean squared error} and $R^2$ instead of accuracy and the Kappa statistic. 

\item However, there are
many models where there is no notion of model degrees of freedom (such as random forests)
or where there are more parameters than training set samples. 

\item Given this, \texttt{train} ignores
degrees of freedom when computing performance values. For example, to compute $R^2$, the
correlation coeficient is computed between the observed and predicted values and squared.

\item When comparing models, the performance metrics will be on the same scale, but these metrics
do not penalize model complexity (as \textit{adjusted} $R^2$ does) and will tend to favor more complex
fits over simpler models.
\end{itemize}
%----------------------------------------------------------%
\subfile{caret-roccurve}
\subfile{caret-resampling}
%----------------------------------------------------------%
\end{document}
