<html>

<head>
<style type="text/css">
.knitr .inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
}
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.rimage .left {
  text-align: left;
}
.rimage .right {
  text-align: right;
}
.rimage .center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}
</style>
<title>Testing for Outliers</title>
</head>

<body>

<h2>Grubbs' Test</h2>
Grubbs' test is a formal hypothesis test for assessing whether or not a  data set contains an outlier.
This data set is univariate and approximately normal distributed. This test is designed for assessing one outlier only.  If more outliers are suspected, alternative tests, such as the Tietjen-Moore test, are recommended.

<h4>Hypotheses:</h4> Grubbs' test is defined for the hypothesis: 


Ho :  There are no outliers in the data set  
Ha :  There is exactly one outlier in the data set  



<div class="chunk" id="unnamed-chunk-1"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl com"># install.packages(&quot;outliers&quot;)</span>
<span class="hl kwd">library</span><span class="hl std">(outliers)</span>
</pre></div>
<div class="error"><pre class="knitr r">## Error in library(outliers): there is no package called 'outliers'
</pre></div>
<div class="source"><pre class="knitr r"><span class="hl com">#Package Author : Lukasz Komsta (UMLUB, Poland)</span>

<span class="hl std">X</span> <span class="hl kwb">&lt;-</span> <span class="hl kwd">c</span><span class="hl std">(</span><span class="hl num">1</span><span class="hl opt">:</span><span class="hl num">10</span><span class="hl std">,</span><span class="hl num">20</span><span class="hl std">)</span>
<span class="hl kwd">grubbs.test</span><span class="hl std">(X)</span>
</pre></div>
<div class="error"><pre class="knitr r">## Error in eval(expr, envir, enclos): could not find function &quot;grubbs.test&quot;
</pre></div>
</div></div>

<p>You can also embed plots, for example:</p>

<div class="chunk" id="unnamed-chunk-2"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">plot</span><span class="hl std">(cars)</span>
</pre></div>
</div><div class="rimage default"><img src="figure/unnamed-chunk-2-1.png" title="plot of chunk unnamed-chunk-2" alt="plot of chunk unnamed-chunk-2" class="plot" /></div></div>

	
<br>
	<h2>Assumption of Normality</h2>

One of the assumptions of many statistical procedures (including the t- test) is that the population from which you are sampling is normally distributed. The t-test is said to be rather ‘robust’ in terms of this assumption, which means that reality can deviate from this assumption a fair amount without seriously affecting the validity of the analysis. 

This is particularly true when the size of the samples is large (thanks to the Central Limit Theorem). Some deviations from normality can pose a problem for the t-test, specifically those that involve getting extreme scores more frequently than you would if the distribution were normal. 
Statistical Software Packages provides two statistical tests for deviation from normality, the 'Kolomogorov-Smirnov' family of tests and the 'Shapiro-Wilk' test. 
The 'Kolomogorov-Smirnov' test can be used to test if two data sets are distributed according to the same distribution. It can also be used to test if one data set comes from a specified distribution, such as the normal distribution. ( As such, the normal distribution must be specified as an argument to the function.)
For the purposes of this module, we will only use a special case of the 'Kolomogorov-Smirnov' test, known as the ‘ Anderson-Darling' test of normality.
The ‘Anderson-Darling’ test can not be implemented directly in R. Using the test requires the installation of the nortest package. We will look at packages in greater detail later in the semester.

The null hypothesis of both the `Anderson-Darling’ and `Shapiro-Wilk’ tests is that the population is normally distributed, and the alternative hypothesis is that the data is not normally distributed. 




Let us use both tests to assess whether the titration data set (the combined scores from all four students as one data set) is normally distributed.
Judging by this histogram – do you think the data set is normally distributed? 
(Remark : it is skewed to the right)
 
 

Using the ‘Shapiro-Wilk’ Test
> Ttrns = c(X.A, X.B, X.C, X.D)
> shapiro.test(Ttrns)

        Shapiro-Wilk normality test

data:  Ttrns 
W = 0.9188, p-value = 0.09394

Using the ‘Anderson-Darling’ Test
<pre><code>
> library(nortest)
> ad.test(Ttrns)

        Anderson-Darling normality test

data:  Ttrns 
A = 0.6961, p-value = 0.0583
</code></pre>
In both cases we fail to reject the null hypothesis that the data set is normally distributed.
However, the p-values were still quite low in both cases.
 
<h3>Limitations of Tests</h3>
There are some important limitations to the usefulness of these tests.

If you reject H0 you can conclude that the population is not normally distributed, but if you don't reject H0 then you only conclude that you failed to show the population is not normally distributed. In other words, you can prove the population is not normally distributed but you can't prove it is normally distributed.

Rejecting H0 means that the population is not normally distributed, but it doesn't tell you whether it is because it is a fat-tailed distribution, a thin-tailed distribution, a skewed distribution, or something else. 

The tests are influenced by power. If you have a small sample the test may not have enough power to detect non-normality in the population.


</body>
</html>
