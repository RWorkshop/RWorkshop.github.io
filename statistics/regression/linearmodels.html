
<!-----########################################################################################--->
<h3> Simple Linear Regression</h3>


You're probably thinking that it would be a mistake to use simple linear regression considering the plot we just produced. Well, there are ways of transforming data so that we can use linear regression methods. We'll fit a Michaelis-Menten function to these data. Using our data, the form of this function is as follows: 
<p>
Vm and K are the parameters we are interested in estimating from these data. With a little bit of algebraic gymnastics, we can get the above equation to look like this: 
<p>
It may not look like this helped, but it did. If you look closely, you'll see that this has the form of a simple linear regression model. Making these substitutions, conc/vel=ytrans, K/Vm=a, and 1/Vm=b, the equation becomes: 

Here's our game plan. We'll first create the new transformed variable ytrans, then fit the linear regression to estimate the parameter a and b. Then we can calculate Vm and K from there. 
We can easily add another variable to our data frame: 
> df$ytrans <- df$conc/df$vel
Take a look at df and see that there is a new column added. Plot the new variable against conc to check whether a linear regression model is appropriate. 
> plot(df$conc, df$ytrans)
Now we are ready to fit our regression model. We'll use the function lm(), which stands for linear model. 
<pre><code>
> lmfit <- lm(ytrans~conc, data=df)
</code></pre>
<p>
By default, you will get an error if there are any missing values in your data when you run this function. If this happens, you may want to omit those cases that contain missing values and fit the model on the remaining cases. To do so, run the same function with an argument to specify the desired action. 
<pre><code>
> lmfit <- lm(ytrans~conc, data=df, na.action=na.omit)
</code></pre>
<!-----########################################################################################--->

This might make more sense if you know that R designates missing values by NA. Now let's look at this function call. The desired model is specified with ytrans~conc. Think of the tilde as an equal sign when specifying a model. ytrans is our response, so it goes on the left side. conc is our explanatory variable, so it goes on the right. An intercept term is assumed so you do not need to include it in the model definition. However, it is possible to force a zero intercept if you wanted. 
Notice that there was not any output automatically generated when you fit the regression. The results have been saved in an object we called lmfit. This object is actually a list that contains several objects, which you can see with the function names() 

<pre><code>
> names(lmfit)
 [1] "coefficients"  "residuals"     "effects"       "rank"      
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "xlevels"       "call"          "terms"         "model"  
</code></pre>
Any of these terms can be viewed or used with the same method you used to view a variable in a data frame: object name, followed by a dollar sign, then the element name. For example, lmfit$call. If you want to view most of the standard regression output, use the summary() function: 

<pre><code>	
> summary(lmfit)

Call:
lm(formula = ytrans ~ conc, data = df)

Residuals:
         1          2          3          4          5          6          7 
 8.333e-04 -1.727e-03 -1.864e-04  5.013e-04  1.363e-04  9.112e-05  3.515e-04 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 0.004303   0.000451   9.541 0.000214 ***
conc        0.259603   0.003102  83.703 4.61e-09 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Residual standard error: 0.0009071 on 5 degrees of freedom
Multiple R-Squared: 0.9993,     Adjusted R-squared: 0.9991 
F-statistic:  7006 on 1 and 5 DF,  p-value: 4.612e-09 
</code></pre>
You can also easily view diagnostic plots (residuals, observed vs fitted, Cook's distance, etc.): 
> plot(lmfit)
R knows that this is the output from a linear model and will generate the appropriate plots. How does it know? Check to see what class it is: 
> class(lmfit)
[1] "lm"
You can get the coefficients from your model fit with the coef() function: 
> coef(lmfit)
(Intercept)        conc 
0.004303145 0.259602617
These are estimates for the parameters we called a and b, respectively. The output from coef() is just a vector of length 2. Let's use the coefficients to add a line to our graph of ytrans vs conc: 
<pre><code>
> plot(df$conc, df$ytrans)
> abline(coef(lmfit))
</code></pre>
Here's what it looks like:

The last thing we have to do is back-calculate to get our non-linear parameters, Vm and K: 
<pre><code>
> Vm <- 1/coef(lmfit)[2]
> K <- Vm*coef(lmfit)[1]
> Vm
    conc 
3.852041 
> K
      conc 
0.01657589 
</code></pre>
Remember that the output from the coef function is a vector of length 2 so we can access the desired coefficient using the brackets as shown. We now have our parameter estimates! 

	
<h3>Linear Regression Model</h3>

A linear relationship can be defined by the simple linear regression model
\[y = \beta_0 + \beta_1x + \epsilon\]

The intercept $\beta_0$ describes the point at which the line intersects the y axis.
The slope $\beta_1$ describes the change in ‘y’ for every unit increase in the predictor variable $x$.

From the data set, we determine the regression coefficients, i.e. estimates for slope and intercept. (N.B. There are variations on this notation in textbooks).

<ul>	
<li> $b_0$ : the intercept estimate.</li>
<li> $b_1$ : the slope estimate.</li>
</ul>
Therefore the fitted model can be expressed as

\[ \hat{y} = b_0 + b_1x \]
Recall $\hat{y}$  denotes the predicted value for y, given some value x.

<!---#########################################################################################--->
<h4>Fitting a Model with \texttt{R}</h4>

The  R command <tt>lm()</tt> is used to fit linear models. Firstly the response variable $y$  is specified, then the predictor variable $x$.

The tilde sign is used to denote the dependent relationship (i.e. y depends on x). The regression coefficients are then determined.

<pre><code>
lm(Y~X) # y depends on X
</code></pre>

The output will include the formula, and two coefficient terms

<ul>
<li> The intercept estimate is recorded under $(Intercept)$
<li> The slope estimate is recorded under the name of the predictor variable (here : $X$ ).
</ul>	
	
<pre>
Call:
lm(formula = Y ~ X)

Coefficients:
(Intercept)            X
     0.7812       0.8581
</pre>

A more detailed data output (i.e. more than just the coefficients) is generated in the form of a data object, using the \textbf{\texttt{summary()}} command.
