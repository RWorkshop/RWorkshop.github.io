
\section*{Testing for Normality}
\subsection*{Introduction}
An assessment of the normality of data is a prerequisite for many statistical tests because normal data is an underlying assumption in \textbf{\textit{parametric testing}}. There are two main methods of assessing normality: graphically and numerically.

This set of procedures will help you to determine whether your data is normal, and therefore, that this assumption is met in your data for statistical tests. The approaches can be divided into two main themes: relying on statistical tests or visual inspection. 

Statistical tests have the advantage of making an objective judgement of normality, but are disadvantaged by sometimes not being sensitive enough at low sample sizes or overly sensitive to large sample sizes. 

\subsection*{Graphical Methods}
As such, some statisticians prefer to use their experience to make a subjective judgement about the data from plots or graphs. Graphical interpretation has the advantage of allowing good judgement to assess normality in situations when numerical tests might be over or under sensitive, but graphical methods do lack objectivity. If you do not have a great deal of experience interpreting normality graphically, it is probably best to rely on the numerical methods.

\section{Testing The Assumption of Normality}
\begin{itemize}
\item A fundamental assumption of many statistical procedures is that some or all variables are normally distributed. When testing the mean of a variable, it is assumed that the variable is normally distributed. 
\item An important assumption for linear models (i.e. regression models) is that the residuals (differences between observed and predicted value) are normally distributed with mean zero.

\item It is important to test that the data is normally distributed before embarking on any further analysis. 
\item There are two formal hypothesis tests that can be used to assess normality.
\begin{itemize}
	\item[$\ast$] The Anderson-Darling Test
	\item[$\ast$] The Shapiro-Wilk Test
\end{itemize}
\item Shapiro-Wilk test is considered more accurate with smaller datasets (i.e. less than approximately 100). 
\item However, both are commonly reported together in statistical reporting. We will just use the Shapiro-Wilk test for this module.
\end{itemize}
\subsection{Hypotheses}
The null hypothesis of both the `Anderson-Darling' and `Shapiro-Wilk' tests is that the population is normally distributed, and the alternative hypothesis is that the data is not normally distributed. For both tests, the null and alternative hypothesis are :\\
\begin{description}
\item[$H_0 : $] The data set is normally distributed.\\
\item[$H_1 : $] The data set is \textbf{not} normally distributed.\\
\end{description}

\noindent \textbf{Importantly} - this test is used to test for "non-normality". Essentially the question is to establish if there is enough evidence to show that the data is NOT normally distributed. 
%---------------------------------------------------------------------- %
\newpage
\subsection{Anderson-Darling Test}
To implement the Anderson-Darling Test for Normality, one must first install the \textbf{\emph{nortest}} package.

\begin{framed}
\begin{verbatim}
library(nortest)
#Generate 100 normally distributed random numbers
set.seed(1234)
NormDat = rnorm(100)
ad.test(NormDat)
\end{verbatim}
\end{framed}
\subsection{Shapiro-Wilk Test}
The Shapiro-Wilk test is directly implementable, without loading any additional packages.

\begin{framed}
\begin{verbatim}
#Generate 100 normally distributed random numbers
set.seed(1234)
NormDat = rnorm(100)

shapiro.test(NormDat)
\end{verbatim}
\end{framed}
Sample output, using the randomly generated \texttt{NormDat} data set, is as follows:
\begin{framed}
\begin{verbatim}
> shapiro.test(NormDat)

        Shapiro-Wilk normality test

data:  NormDat
W = 0.9864, p-value = 0.4003
\end{verbatim}
\end{framed}
Here, the p-value is well above the 0.05 threshold. Hence we \textbf{fail to reject} the null hypothesis, and may proceed to treat the \texttt{NormDat} data set as normally distributed.
\newpage
\subsection{Graphical Procedures for Assessing Normality}
There are two useful graphical methods for determining whether a data set was normally distributed. The first is the histogram, which we have seen previously. If the histogram is reasonably bell-shaped, then the data can be assumed to be normally distributed. The relevant \texttt{R} command is \texttt{\textbf{hist()}}.


The second is the \textbf{\emph{quantile-quantile plot}} (or QQ-plot).
For assessing normality, we implement a qq-plot  using the \texttt{\textbf{qqnorm()}} function.

Additionally the command \texttt{\textbf{qqline()}} function adds a trendline to a normal quantile-quantile plot. If the data is normally distributed, then the points on the plot follow the trendline.

\begin{framed}
\begin{verbatim}
#Generate 100 normally distributed random numbers

NormDat = rnorm(100)

qqnorm(NormDat)
qqline(NormDat)
\end{verbatim}
\end{framed}

% Section 8 Testing Normality
\subsection{Transforming the Data}
\begin{itemize}
\item Sometimes when we get non-normal data, it may be possible that we can change the scale of our data i.e. transform it to get a normal distribution. 
\item One transformation that often (but not always) works for positively skewed data is the natural logarithm (ln) transformation.
\item In such a case, we work with the natural logarithms of the data set, rather than the data itself.
\end{itemize}

\begin{framed}
\begin{verbatim}
> set.seed(1919)
> X = rexp(30,rate=0.50)
> shapiro.test(X)

        Shapiro-Wilk normality test

data:  X
W = 0.8213, p-value = 0.0001646
> shapiro.test(log(X))

        Shapiro-Wilk normality test

data:  log(X)
W = 0.9402, p-value = 0.09206
\end{verbatim}
\end{framed}
\subsection*{Outliers}
\begin{itemize}
\item Another reason that the data may not be normally distributed is the presence of an outlier. 
\item We shall look at formal tests for outliers (such as the \textbf{Grubb's Outliers test}) later. 
\item Recall that boxplots can be used to detect potential outliers.
\end{itemize}
